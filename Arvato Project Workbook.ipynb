{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from kmodes.kmodes import KModes\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data, specifying datatypes for columns 18 and 19, which have mixtures of datatypes, to speed up import\n",
    "azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';', dtype={18: object, 19: object})\n",
    "customers = pd.read_csv('data/Udacity_CUSTOMERS_052018.csv', sep=';', dtype={18: object, 19: object})\n",
    "\n",
    "# Create backup copies to avoid having to reimport\n",
    "azdias_copy = azdias.copy()\n",
    "customers_copy = customers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of customer data\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of azdias data\n",
    "azdias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find names of problematic 18th and 19th columns\n",
    "customers.columns[18], customers.columns[19]\n",
    "# According to the schema, these columns are the New German CAMEO Typology established together with Call Credit in late 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also according to the schema, there should be an additional associated column named. Finding this to investigate this column too\n",
    "cameo_cols = [col for col in customers.columns if 'CAMEO' in col]\n",
    "print(cameo_cols)\n",
    "# 3rd Cameo column is CAMEO_DEU_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First problematic column contains numbers and NaN values\n",
    "customers['CAMEO_DEUG_2015'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second problematic column contains numbers and NaN values\n",
    "customers['CAMEO_INTL_2015'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The associated 3rd columm is a mixture of number-letter combos and NaN values\n",
    "customers['CAMEO_DEU_2015'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change columns 18 and 19 to type float\n",
    "customers['CAMEO_DEUG_2015'] = pd.to_numeric(customers['CAMEO_DEUG_2015'], errors='coerce')\n",
    "customers['CAMEO_INTL_2015'] = pd.to_numeric(customers['CAMEO_INTL_2015'], errors='coerce')\n",
    "azdias['CAMEO_DEUG_2015'] = pd.to_numeric(azdias['CAMEO_DEUG_2015'], errors='coerce')\n",
    "azdias['CAMEO_INTL_2015'] = pd.to_numeric(azdias['CAMEO_INTL_2015'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract letter from CAMEO_DEU_2015 into new column and remove original which is duplicate of CAMEO_DEUG_2015\n",
    "customers['CAMEO_DEU_2015_let'] = customers['CAMEO_DEU_2015'].str[1]\n",
    "customers = customers.drop(columns = 'CAMEO_DEU_2015', axis=1)\n",
    "azdias['CAMEO_DEU_2015_let'] = azdias['CAMEO_DEU_2015'].str[1]\n",
    "azdias = azdias.drop(columns = 'CAMEO_DEU_2015', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could show they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could show missing data is -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on schema provided, the majority of missing data is indicated by -1\n",
    "# Ahead of imputation, transform these -1 values to NaNs\n",
    "customers = customers.replace({-1:np.nan})\n",
    "azdias = azdias.replace({-1:np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and visualise columns with most missing data in customers dataset\n",
    "round(customers.isnull().sum(axis = 0)/customers.shape[0]*100,2).sort_values(ascending = False).head(20).plot(kind = 'bar', figsize=(20,10))\n",
    "plt.title(\" % of missing values per column in customers dataset\", fontdict={'fontsize': 16})\n",
    "plt.ylabel('% of missing values', fontdict={'fontsize': 12})\n",
    "plt.xlabel('columns in dataset', fontdict={'fontsize': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with most missing data in azdias dataset\n",
    "round(azdias.isnull().sum(axis = 0)/azdias.shape[0]*100,2).sort_values(ascending = False).head(20).plot(kind = 'bar', figsize=(20,10))\n",
    "plt.title(\" % of missing values per column in azdias dataset\", fontdict={'fontsize': 16})\n",
    "plt.ylabel('% of missing values', fontdict={'fontsize': 12})\n",
    "plt.xlabel('columns in dataset', fontdict={'fontsize': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with a high proportion of missing data\n",
    "empty_cols = customers.columns[customers.isnull().sum(axis = 0)/customers.shape[0]*100 > 50]\n",
    "customers = customers.drop(columns = empty_cols, axis=1)\n",
    "\n",
    "empty_cols = azdias.columns[azdias.isnull().sum(axis = 0)/azdias.shape[0]*100 > 50]\n",
    "azdias = azdias.drop(columns = empty_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with a high proportion of missing data\n",
    "empty_rows = customers[customers.isnull().sum(axis = 1)/customers.shape[1]*100 > 80].index\n",
    "customers = customers.drop(empty_rows, axis=0)\n",
    "\n",
    "empty_rows = azdias[azdias.isnull().sum(axis = 1)/azdias.shape[1]*100 > 80].index\n",
    "azdias = azdias.drop(empty_rows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function which fills missing values with mode - using mode as data is categorical \n",
    "# e.g. ages are grouped into ranges rather than a field holding the age of an individual \n",
    "fill_mode = lambda col: col.fillna(col.mode()[0])\n",
    "# Apply to all columns in customers dataset\n",
    "customers = customers.apply(fill_mode, axis=0)\n",
    "# Apply to all columns in azdias dataset\n",
    "azdias = azdias.apply(fill_mode, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode categorical data\n",
    "object_columns = customers.columns[customers.dtypes == object]\n",
    "customers_clean = pd.get_dummies(data=customers, columns=object_columns)   \n",
    "\n",
    "object_columns = azdias.columns[azdias.dtypes == object]\n",
    "azdias_clean = pd.get_dummies(data=azdias, columns=object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of missing values in customers dataset\n",
    "round(customers_clean.isnull().sum(axis = 0)/customers_clean.shape[0]*100,2).sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of missing values in azdias dataset\n",
    "round(azdias_clean.isnull().sum(axis = 0)/azdias_clean.shape[0]*100,2).sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put cleaning steps in function\n",
    "def clean_data(df):\n",
    "    '''\n",
    "     INPUT:\n",
    "     customers_df - dataframe of customer data\n",
    "     genpop_df - dataframe of data for the general population\n",
    "     OUTPUT:\n",
    "     customers_df_clean - dataframe of customer data ready for segmentation\n",
    "     genpop_df_clean - dataframe of data for the general population ready for segmentation\n",
    "    \n",
    "     Removes columns which aren't present in both dataframes\n",
    "     Sorts columns with incorrect datatypes\n",
    "     Extract letter field from CAMEO_DEU_2015\n",
    "     Removes columns with high proportion of missing data\n",
    "     Impute missing catgorical data\n",
    "     '''\n",
    "    \n",
    "    # Sort columns with incorrect datatypes\n",
    "    df['CAMEO_DEUG_2015'] = pd.to_numeric(df['CAMEO_DEUG_2015'], errors='coerce')\n",
    "    df['CAMEO_INTL_2015'] = pd.to_numeric(df['CAMEO_INTL_2015'], errors='coerce')\n",
    "    for col in (['CAMEO_DEUG_2015', 'CAMEO_INTL_2015']):\n",
    "        df[col] = pd.to_numeric(df['CAMEO_DEUG_2015'], errors='coerce')\n",
    "    # Extract letter field from CAMEO_DEU_2015 and remove original, which is a duplicate of CAMEO_DEUG_2015\n",
    "    df['CAMEO_DEU_2015_let'] = df['CAMEO_DEU_2015'].str[1]\n",
    "    df.drop(columns = 'CAMEO_DEU_2015', axis=1)\n",
    "    # Change -1 values to Nans\n",
    "    df = df.replace({-1:np.nan})\n",
    "    # Remove EINGEFUEGT_AM which doesn't seem very helpful\n",
    "    df = df.drop(columns = ['EINGEFUEGT_AM'], axis=1)\n",
    "    # Remove columns with a high proportion of missing data\n",
    "    empty_cols = df.columns[df.isnull().sum(axis = 0)/df.shape[0]*100 > 50]\n",
    "    df = df.drop(columns = empty_cols, axis=1)\n",
    "    # Remove rows with a high proportion of missing data\n",
    "    empty_rows = df[df.isnull().sum(axis = 1)/df.shape[1]*100 > 80].index\n",
    "    df = df.drop(empty_rows, axis=0)\n",
    "    # Impute missing data with mode\n",
    "    # Create function which fills missing values with mode\n",
    "    fill_mode = lambda col: col.fillna(col.mode()[0])\n",
    "    # Apply to all columns\n",
    "    df = df.apply(fill_mode, axis=0)\n",
    "    ## Sort categorical data\n",
    "    # One hot encode categorical data\n",
    "    object_columns = df.columns[df.dtypes == object]\n",
    "    df_clean = pd.get_dummies(data=df, columns=object_columns)\n",
    "\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check function is working\n",
    "customers = customers_copy\n",
    "azdias = azdias_copy\n",
    "customers_clean = clean_data(customers)\n",
    "azdias_clean = clean_data(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(customers_clean.isnull().sum(axis = 0)/customers_clean.shape[0]*100,2).sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(azdias_clean.isnull().sum(axis = 0)/azdias_clean.shape[0]*100,2).sort_values(ascending = False).head(10)\n",
    "# There are no longer any missing values in the datasets suggesting the function is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns which aren't present in both dataframes ahead of segmentation\n",
    "uncommon_cols = set(customers_clean.columns).symmetric_difference(set(azdias_clean.columns))\n",
    "customers_clean = customers_clean.drop(columns = uncommon_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_clean.shape\n",
    "# Both datasets have the same number of rows and so are ready for segmenatation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Old function\n",
    "# def prepare_seg_data(customers_df, genpop_df):\n",
    "#     '''\n",
    "#      INPUT:\n",
    "#      customers_df - dataframe of customer data\n",
    "#      genpop_df - dataframe of data for the general population\n",
    "#      OUTPUT:\n",
    "#      customers_df_clean - dataframe of customer data ready for segmentation\n",
    "#      genpop_df_clean - dataframe of data for the general population ready for segmentation\n",
    "    \n",
    "#      Removes columns which aren't present in both dataframes\n",
    "#      Sorts columns with incorrect datatypes\n",
    "#      Extract letter field from CAMEO_DEU_2015\n",
    "#      Removes columns with high proportion of missing data\n",
    "#      Impute missing catgorical data\n",
    "#      '''\n",
    "     \n",
    "#     for df in (customers_df, genpop_df):\n",
    "#         # Sort columns with incorrect datatypes\n",
    "#         df['CAMEO_DEUG_2015'] = pd.to_numeric(df['CAMEO_DEUG_2015'], errors='coerce')\n",
    "#         df['CAMEO_INTL_2015'] = pd.to_numeric(df['CAMEO_INTL_2015'], errors='coerce')\n",
    "#         #for col in (['CAMEO_DEUG_2015', 'CAMEO_INTL_2015']):\n",
    "#         #    df[col] = pd.to_numeric(df['CAMEO_DEUG_2015'], errors='coerce')\n",
    "#         # Extract letter field from CAMEO_DEU_2015 and remove original, which is a duplicate of CAMEO_DEUG_2015\n",
    "#         df['CAMEO_DEU_2015_let'] = df['CAMEO_DEU_2015'].str[1]\n",
    "#         df.drop(columns = 'CAMEO_DEU_2015', axis=1)\n",
    "#         # Change -1 values to Nans\n",
    "#         df = df.replace({-1:np.nan})\n",
    "#         # Remove columns with a high proportion of missing data\n",
    "#         empty_cols = df.columns[df.isnull().sum(axis = 0)/df.shape[0]*100 > 50]\n",
    "#         df = df.drop(columns = empty_cols, axis=1)\n",
    "#         # Remove rows with a high proportion of missing data\n",
    "#         empty_rows = df[df.isnull().sum(axis = 1)/df.shape[1]*100 > 80].index\n",
    "#         df = df.drop(empty_rows, axis=0)\n",
    "#         ## Sort categorical data\n",
    "#         # One hot encode categorical data\n",
    "#         object_columns = df.columns[df.dtypes == object]\n",
    "#         df = pd.get_dummies(data=df, columns=object_columns)\n",
    "#         # Impute missing data with mode\n",
    "#         # Create function which fills missing values with mode\n",
    "#         fill_mode = lambda col: col.fillna(col.mode()[0])\n",
    "#         # Apply to all columns\n",
    "#         df = df.apply(fill_mode, axis=0)\n",
    "        \n",
    "        \n",
    "#     # Removes columns which aren't present in both dataframes\n",
    "#     uncommon_cols = set(customers_df.columns).symmetric_difference(set(genpop_df.columns))\n",
    "#     customers_df = customers_df.drop(columns = uncommon_cols, axis=1)\n",
    "    \n",
    "#     customers_df_clean = customers_df\n",
    "#     genpop_df_clean = genpop_df\n",
    "    \n",
    "#     return customers_df_clean, genpop_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution of a subset of columns based on code on https://towardsdatascience.com/a-guide-to-pandas-and-matplotlib-for-data-exploration-56fad95f951c\n",
    "customers_subset = customers_clean.iloc[: ,1:10]\n",
    "sns.pairplot(customers_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even when looking at the first 10 columns many are not normally distributed\n",
    "# Many clustering algorithms require features to be normally distributed therefore use apply scaler to standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and fit scaler to \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(customers_clean)\n",
    "customers_scaled_features = scaler.transform(customers_clean)\n",
    "customers_scaled = pd.DataFrame(customers_scaled_features, columns=customers_clean.columns)\n",
    "customers_scaled.head()\n",
    "\n",
    "# Apply scaler to customers dataset\n",
    "azdias_scaled_features = scaler.transform(azdias_clean)\n",
    "azdias_scaled = pd.DataFrame(azdias_scaled_features, columns=azdias_clean.columns)\n",
    "azdias_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of columns in the dataset and it is likely that a lot will correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of columns in the dataset and it is likely that a lot will correlate\n",
    "# Create visualisation of correlations between columns based on code on https://towardsdatascience.com/a-guide-to-pandas-and-matplotlib-for-data-exploration-56fad95f951c\n",
    "corr = customers_subset.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap=sns.diverging_palette(220, 10, as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the visualisation above, ANZ_STATISTISCHE_HAUSHALTE correlates strongly and somewhat with \n",
    "#ANZ_HAUSHALTE_AKTIV and ANZ_HH_TITEL\n",
    "# Therefore apply principle component analysis to reduce dimensionality of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of components to use using link: https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
    "#Fitting the PCA algorithm with our Data\n",
    "pca = PCA().fit(customers_scaled)\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Pulsar Dataset Explained Variance')\n",
    "plt.show()\n",
    "# Plot suggests 250 components descripe 90% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to azdias dataset\n",
    "pca = PCA(n_components=250)\n",
    "customers_components = pca.fit_transform(customers_scaled)\n",
    "\n",
    "# Apply to customers datast\n",
    "azdias_components = pca.transform(azdias_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kmeans clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine number of clusters using elbow method - https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3\n",
    "sse = []\n",
    "for k in range(1,10):\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\")\n",
    "    kmeans.fit(customers_components)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(12,6))    \n",
    "plt.plot(range(1,10), sse)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "# 6 clusters looks optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6)\n",
    "customers_scaled['clusters'] = kmeans.fit_predict(customers_components)\n",
    "azdias_scaled['clusters'] = kmeans.predict(azdias_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise clustering results to see if there was a non-Euclidean shape to the data that k-means failed to pick up on\n",
    "tsne = TSNE(n_components=2, random_state=1986)\n",
    "twodim_arr = tsne.fit_transform(customers_scaled.iloc[:,:-1])\n",
    "\n",
    "color = iter(plt.cm.rainbow(np.linspace(0,1,3)))\n",
    "\n",
    "for group in list(customers_scaled['clusters'].unique().tolist()):\n",
    "    c = next(color)\n",
    "    plt.scatter(twodim_arr[customers_scaled['clusters'] == group, 0], \n",
    "                twodim_arr[customers_scaled['clusters'] == group, 1],\n",
    "                color=c,\n",
    "                label=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots showing how the clusters differ for each of the KPIs in the dataset\n",
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(12) \n",
    "\n",
    "for i, kpi in enumerate(customers_scaled.iloc[:,:-1].columns.tolist()):\n",
    "    axes[i].set_title(kpi)\n",
    "    data = list()\n",
    "    for j in range(3):\n",
    "        data.append(customers_scaled[customers_scaled['clusters'] == j][kpi])\n",
    "    axes[i].boxplot(data, labels=['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distribution of customer and genpop individuals across clusters\n",
    "customer_perc = customers_scaled['clusters'].value_counts()/customers_scaled['clusters'].shape[0]*100\n",
    "gen_pop_perc = azdias_scaled['clusters'].value_counts()/azdias_scaled['clusters'].shape[0]*100\n",
    "distributions = {'Customers': customer_perc, 'Genpop': gen_pop_perc}\n",
    "dists = pd.DataFrame(data=distributions)\n",
    "\n",
    "# Visualise distributions\n",
    "dists.plot(kind = 'bar', figsize=(20,10))\n",
    "plt.ylabel(\"Percentage of population\", fontdict={'fontsize': 12})\n",
    "plt.xlabel(\"Cluster number\", fontdict={'fontsize': 12})\n",
    "plt.title(\"Distribution of customer and general population data across clusters\", fontdict={'fontsize': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers are more likely to be in clusters 0 and 3 and less likely to be in cluster 5 than the general population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find average score for clusters 0 and 3\n",
    "cust_avg = customers_scaled[customers_scaled['clusters'].isin([0, 3])].mean()\n",
    "genpop_avg = azdias_scaled[azdias_scaled['clusters'].isin([0, 3])].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate difference between customer score and gen pop score\n",
    "customer_scores = pd.concat([cust_avg, genpop_avg], axis=1).rename(columns={0: \"customers\", 1: \"genpop\"})\n",
    "customer_scores['diff'] = abs(customer_scores['customers'] - customer_scores['genpop'])\n",
    "customer_scores['diff'].sort_values(ascending = False).head(20).plot(kind = 'bar', figsize=(20,10))\n",
    "plt.ylabel(\"Difference in average score\", fontdict={'fontsize': 12})\n",
    "plt.xlabel(\"Features\", fontdict={'fontsize': 12})\n",
    "plt.title(\"Chart to demonstrate the features which differ most between Customer and General Population\", fontdict={'fontsize': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, it is clear that the biggest differentiators between customers and general pop are listed below with their meaning. Many fields are not in the data descriptions and so are missing meanings.\n",
    "'LNR'\n",
    "'AKT_DAT_KL'\n",
    "'VK_ZG11'\n",
    "'VK_DISTANZ'\n",
    "'D19_KONSUMTYP' - Consumption type\n",
    "'CJT_TYP_3'- Customer Journey Typology - advertisinginterested Store-shopper\n",
    "'CJT_TYP_5' - Customer Journey Typology - Advertising- and Cross-Channel-Enthusiast\n",
    "'CJT_TYP_6' - Customer Journey Typology - Advertising-Enthusiast with restricted Cross-Channel-Behaviour \n",
    "'ALTERSKATEGORIE_FEIN'- Age classification - Higher = holder\n",
    "'CJT_TYP_4' - Customer Journey Typology - advertisinginterested Online-shopper\n",
    "'EINGEZOGENAM_HH_JAHR' - Potentially related to year of birth\n",
    "'VK_DHT4A',\n",
    "'PRAEGENDE_JUGENDJAHRE' - dominating movement in the person's youth (avantgarde or mainstream) - higher score = later \n",
    "'WOHNDAUER_2008' - Length of residence = higher = later\n",
    "'FINANZ_MINIMALIST' - financial typology: low financial interest = higher score = low interest(?)\n",
    "'CJT_KATALOGNUTZER'\n",
    "'SEMIO_TRADV' - affinity indicating in what way the person is traditional minded higher score = low affinity\n",
    "'RT_SCHNAEPPCHEN'\n",
    "'FINANZ_VORSORGER' - financial typology: be prepared = higher score = low\n",
    "'HH_EINKOMMEN_SCORE' - estimated household net income - higher score = low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mailout_train['RESPONSE']\n",
    "X = mailout_train.drop(columns = ['RESPONSE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare various models to find the one which gives highest accuracy using https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model in turn\n",
    "seed = 10\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer â€“ this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
